{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Loading Imports\n",
      "---Success\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "print('---Loading Imports')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print('---Success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Importing the uci dataset\n",
      "---Success\n",
      "----------------------------\n",
      "---Loading the training data\n",
      "---Success\n",
      "----------------------------\n",
      "Loading training data lengths\n",
      "x_train_data length:  43500\n",
      "y_train_data length:  43500\n",
      "----------------------------\n",
      "Loading test data lengths\n",
      "x_test_data length:  14500\n",
      "y_test_data length:  14500\n",
      "----------------------------\n",
      "Total data length is: 58000\n"
     ]
    }
   ],
   "source": [
    "# import the uci dataset\n",
    "print('---Importing the uci dataset')\n",
    "uci_train = []\n",
    "training_data_all=[]\n",
    "x_train_data = []\n",
    "y_train_data = []\n",
    "uci_dir = 'uci_dataset'\n",
    "training_data_file = 'shuttle.trn'\n",
    "\n",
    "print('---Success')\n",
    "print('----------------------------')\n",
    "\n",
    "# load the training data\n",
    "print('---Loading the training data')\n",
    "with open('./'.join([os.getcwd(), uci_dir, training_data_file])) as f:\n",
    "    \n",
    "    for line in f:\n",
    "        uci_train.append(line) # for checking if split of data is correct\n",
    "        split = line.strip().split(' ')\n",
    "        training_data_val =[ int(x) for x in split]\n",
    "        x_values = [ float(x) for x in split[:-1]]\n",
    "        y_values = int(split[-1])\n",
    "        \n",
    "        training_data_all.append(training_data_val)\n",
    "        x_train_data.append(x_values)\n",
    "        y_train_data.append([y_values])\n",
    "        \n",
    "print('---Success')\n",
    "print('----------------------------')\n",
    "        \n",
    "print('Loading training data lengths')\n",
    "print('x_train_data length: ', len(x_train_data))\n",
    "print('y_train_data length: ', len(y_train_data))\n",
    "print('----------------------------')\n",
    "\n",
    "\n",
    "# load the training data\n",
    "uci_test = []\n",
    "x_test_data = []\n",
    "y_test_data = []\n",
    "test_data_file = 'shuttle.tst'\n",
    "\n",
    "with open('/'.join([os.getcwd(), uci_dir, test_data_file])) as f:\n",
    "    \n",
    "    for line in f:\n",
    "        uci_test.append(line) # for checking if split of data is correct\n",
    "        split = line.strip().split(' ')\n",
    "        x_values = [ float(x) for x in split[:-1]]\n",
    "        y_values = int(split[-1])\n",
    "        \n",
    "        x_test_data.append(x_values)\n",
    "        y_test_data.append([y_values])\n",
    "\n",
    "print('Loading test data lengths')\n",
    "print('x_test_data length: ', len(x_test_data))\n",
    "\n",
    "print('y_test_data length: ', len(y_test_data))\n",
    "print('----------------------------')\n",
    "\n",
    "print('Total data length is: %d' % (len(x_train_data) + len(x_test_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---randomized training data by reducing class 1\n",
      "============ reorganization dataset======================\n",
      "x_reorganization_data length:  16213\n",
      "y_reorganization_data length:  16213\n",
      "============ training dataset======================\n",
      "Training dataset:\n",
      "class 1: count: 34108 ratio: 78.41%\n",
      "class 2: count: 37 ratio: 0.09%\n",
      "class 3: count: 132 ratio: 0.30%\n",
      "class 4: count: 6748 ratio: 15.51%\n",
      "class 5: count: 2458 ratio: 5.65%\n",
      "class 6: count: 6 ratio: 0.01%\n",
      "class 7: count: 11 ratio: 0.03%\n",
      "============ reorganization dataset======================\n",
      "reorganization dataset:\n",
      "class 1: count: 6821 ratio: 42.07%\n",
      "class 2: count: 37 ratio: 0.23%\n",
      "class 3: count: 132 ratio: 0.81%\n",
      "class 4: count: 6748 ratio: 41.62%\n",
      "class 5: count: 2458 ratio: 15.16%\n",
      "class 6: count: 6 ratio: 0.04%\n",
      "class 7: count: 11 ratio: 0.07%\n",
      "===============count the number of data for each classes - test dataset=====================================\n",
      "Test dataset:\n",
      "class 1: count: 11478 ratio: 79.16%\n",
      "class 2: count: 13 ratio: 0.09%\n",
      "class 3: count: 39 ratio: 0.27%\n",
      "class 4: count: 2155 ratio: 14.86%\n",
      "class 5: count: 809 ratio: 5.58%\n",
      "class 6: count: 4 ratio: 0.03%\n",
      "class 7: count: 2 ratio: 0.01%\n"
     ]
    }
   ],
   "source": [
    "print('---randomized training data by reducing class 1')\n",
    "# reorganization training data\n",
    "count=0\n",
    "x_reorganization_data=[]\n",
    "y_reorganization_data=[]\n",
    "for x,y in zip(x_train_data,y_train_data):\n",
    "    if y[0]==1:\n",
    "        count+=1\n",
    "        if (count%5)==0: \n",
    "            x_reorganization_data.append(x)\n",
    "            y_reorganization_data.append(y)\n",
    "       #     print('append 1')\n",
    "       # print('y==1',count,' - ',y,'not append 1')\n",
    "    else:\n",
    "        x_reorganization_data.append(x)\n",
    "        y_reorganization_data.append(y)\n",
    "       # print('y!=1',count,' - ',y,'append others')\n",
    "print('============ reorganization dataset======================')   \n",
    "print('x_reorganization_data length: ',len(x_reorganization_data))\n",
    "print('y_reorganization_data length: ',len(y_reorganization_data))\n",
    "#print(x_reorganization_data[0:10])\n",
    "#print(y_reorganization_data[0:10])\n",
    "\n",
    "print('============ training dataset======================')\n",
    "# count the number of data for each classes - training dataset\n",
    "total_num_data = len(x_train_data)\n",
    "train_bins = [[] for _ in range(7)]\n",
    "for x, y in zip(x_train_data, y_train_data):\n",
    "    train_bins[y[0] - 1].append(x)\n",
    "    \n",
    "print('Training dataset:')\n",
    "for idx, category in enumerate(train_bins):\n",
    "    print('class %d: count: %d ratio: %.2f%%' % (idx + 1, len(category), (len(category)/float(total_num_data)) * 100) )\n",
    "    \n",
    "#print('====data train_bins:====')\n",
    "#print(train_bins)\n",
    "\n",
    "print('============ reorganization dataset======================')\n",
    "total_num_data = len(x_reorganization_data)\n",
    "train_bins = [[] for _ in range(7)]\n",
    "for x, y in zip(x_reorganization_data, y_reorganization_data):\n",
    "    train_bins[y[0] - 1].append(x)\n",
    "    \n",
    "print('reorganization dataset:')\n",
    "for idx, category in enumerate(train_bins):\n",
    "    print('class %d: count: %d ratio: %.2f%%' % (idx + 1, len(category), (len(category)/float(total_num_data)) * 100) )\n",
    "\n",
    "\n",
    "print('===============count the number of data for each classes - test dataset=====================================')\n",
    "# count the number of data for each classes - test dataset\n",
    "total_num_data = len(x_test_data)\n",
    "test_bins = [[] for _ in range(7)]\n",
    "for x, y in zip(x_test_data, y_test_data):\n",
    "    test_bins[y[0] - 1].append(x)\n",
    "    \n",
    "print('Test dataset:')\n",
    "for idx, category in enumerate(test_bins):\n",
    "    print('class %d: count: %d ratio: %.2f%%' % (idx + 1, len(category), (len(category)/float(total_num_data)) * 100) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using an svm with rbf kernel function:')\n",
    "# Use an svm with rbf kernel function\n",
    "svm = SVC(C=20, kernel='rbf', gamma=20)\n",
    "svm.fit(x_train_data, y_train_data)\n",
    "#print(training_data_all)\n",
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "\n",
    "#testing\n",
    "print(svm.predict(x_train_data))\n",
    "print('the SVM Accuracy::',svm.score(x_train_data, y_train_data))\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "print(preprocessing.scale(x_reorganization_data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99260355 0.99408065 0.99223085 0.98927118 0.99296557 0.99555391]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Do cross validation\n",
    "# Possible approaches\n",
    "# 1. k-fold cross validation\n",
    "# 2. Try different algorithm\n",
    "# 3. Under/Oversampling method\n",
    "# 4. Penalized Models (penalized SVM)\n",
    "# 5. Look into Anomaly Detection\n",
    "\n",
    "# Cross Validation techniques (Non-exhaustive methods)\n",
    "# 1. Holdout Method\n",
    "# 2. k-fold cross validation\n",
    "# 3. Stratified k-fold cross validation\n",
    "#    - each fold contains approximately the same \n",
    "#    percentage of samples of each target class\n",
    "#    in the case of prediction problems, the mean response value is approximately equal in all folds\n",
    "# (Exhaustive methods)\n",
    "# 4. Leave-P-Out Cross Validation\n",
    "\n",
    "\n",
    "# K- CV\n",
    "scores = cross_val_score(svm, preprocessing.scale(x_reorganization_data), np.ravel(y_reorganization_data,order='C'), cv=6, scoring='accuracy')\n",
    "\n",
    "#\n",
    "print(scores)\n",
    "# original Imbalanced data\n",
    "# [0.78377757 0.78395771 0.78413793 0.78429343 0.78429343]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99137666 0.99260173 0.9910549  0.99351852 0.99475309]\n",
      "0.9926609781912695\n"
     ]
    }
   ],
   "source": [
    "# K- CV\n",
    "scores = cross_val_score(svm, preprocessing.scale(x_reorganization_data), np.ravel(y_reorganization_data,order='C'), cv=5, scoring='accuracy')\n",
    "\n",
    "#\n",
    "print(scores)\n",
    "# original Imbalanced data\n",
    "# [0.78377757 0.78395771 0.78413793 0.78429343 0.78429343]\n",
    "\n",
    "# decrease class1 in 1/5\n",
    "# [0.98306129 0.98335388 0.97902529 0.98117284 0.98364198]\n",
    "\n",
    "\n",
    "print(scores.mean())\n",
    "# original Imbalanced data\n",
    "# 0.7840920168940516\n",
    "\n",
    "# decrease class1 in 1/5\n",
    "# 0.9820510558559367"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n",
      "the SVM Accuracy:: 0.7915862068965517\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "svm.fit(preprocessing.scale(x_reorganization_data), np.ravel(y_reorganization_data,order='C'))\n",
    "\n",
    "#testing\n",
    "print(svm.predict(x_test_data))\n",
    "print('the SVM Accuracy::',svm.score(x_test_data, y_test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
